{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdee909",
   "metadata": {},
   "source": [
    "# FakeScope Development.ipynb - Complete Guide\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive fake news detection pipeline combining traditional ML (TF-IDF + baselines) with transformer models (DistilBERT) and ensemble methods.\n",
    "\n",
    "## Chronology & Architecture\n",
    "\n",
    "### Phase 1: Data Loading & Preprocessing (Cells 1-17)\n",
    "- **Load datasets**: Two CSV files merged into unified dataset\n",
    "- **Label normalization**: Maps various labels to binary (0=Fake, 1=True)\n",
    "- **Text cleaning**: Removes punctuation, digits, stopwords (NLTK + sklearn + custom)\n",
    "- **Deduplication**: MD5 content hashes to detect exact duplicates\n",
    "- **EDA**: WordClouds, class distribution, top word/bigram frequencies\n",
    "\n",
    "**Key Configuration**:\n",
    "```python\n",
    "TOKEN_PATTERN = r'(?u)\\b\\w\\w+\\b'  # Skip 1-letter tokens\n",
    "MIN_DF = 5                           # Drop very rare tokens\n",
    "MAX_DF = 0.90                        # Drop extremely common tokens\n",
    "NGRAM_RANGE = (1, 2)                 # Unigrams and bigrams\n",
    "```\n",
    "\n",
    "### Phase 2: Train/Test Split (Cells 18-19)\n",
    "- **GroupShuffleSplit**: Prevents data leakage by grouping duplicate articles\n",
    "- **25% test split** with random_state=42 for reproducibility\n",
    "- **Content hash grouping**: Ensures same article variants stay in same split\n",
    "\n",
    "### Phase 3: Feature Extraction (Cell 20)\n",
    "- **TF-IDF vectorization**: max_features=5000, custom stopwords\n",
    "- **Train-only fitting**: Vectorizer fit on X_train, transform on X_test\n",
    "- **Output**: Sparse matrices (train: ~75% samples, test: ~25%)\n",
    "\n",
    "### Phase 4: Baseline Models (Cells 21-34)\n",
    "- **Logistic Regression**: Linear baseline with L2 regularization\n",
    "- **Decision Tree**: Non-linear baseline with max_depth tuning\n",
    "- **Random Forest**: Ensemble baseline with GridSearchCV hyperparameter tuning\n",
    "- **Evaluation**: Accuracy, F1, ROC/AUC, confusion matrices, feature importance\n",
    "- **Best model selection**: Choose by AUC, save to best_baseline_model.joblib\n",
    "\n",
    "### Phase 5: Transformer Models (Cells 35-56)\n",
    "- **DistilBERT fine-tuning**: HuggingFace Transformers with custom tokenization\n",
    "- **2-stage training** (optional):\n",
    "  1. **Stage 1**: Masked Language Modeling (MLM) on unlabeled corpus for domain adaptation\n",
    "  2. **Stage 2**: Fine-tune adapted model on labeled fake news classification\n",
    "- **Training config**: 3 epochs, batch_size=16, learning_rate=2e-5, MPS (Apple Silicon)\n",
    "- **Cross-validation**: 5-fold StratifiedKFold for robust evaluation\n",
    "- **Model persistence**: Save to ./distilbert_fakenews/\n",
    "\n",
    "### Phase 6: Ensemble & Error Analysis (Cells 39-42)\n",
    "- **Soft voting ensemble**:\n",
    "  ```python\n",
    "  ensemble_proba = 0.6 * bert_proba + 0.4 * rf_proba\n",
    "  ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "  ```\n",
    "- **Weight rationale**: 60% transformer (semantic context) + 40% RF (robust features)\n",
    "- **Error analysis**: Identify misclassified examples, show prediction details\n",
    "- **Attention visualization**: Use bertviz to understand model focus\n",
    "\n",
    "### Phase 7: Fact-Checking Integration (Cells 58-70)\n",
    "- **Google Fact Check API**: Query external fact-checkers for claims\n",
    "- **Claim extraction**: Use spaCy to extract sentences >10 words\n",
    "- **Credibility scoring**: Combine model predictions with fact-check verdicts\n",
    "- **Comment generation**: Explanations based on score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721e513",
   "metadata": {},
   "source": [
    "# Practical Usage Guide\n",
    "\n",
    "## Environment Setup\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate  # On Mac/Linux\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Download spaCy model\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Running the Notebook\n",
    "1. Open Development.ipynb in VS Code\n",
    "2. Select Python interpreter: **Python: Select Interpreter** → Choose .venv\n",
    "3. Run cells sequentially from top to bottom\n",
    "4. Monitor console for warnings about data leakage or errors\n",
    "\n",
    "## Expected Runtimes (M4 Mac)\n",
    "- Data loading & preprocessing: ~2 min\n",
    "- Baseline training: ~5 min\n",
    "- DistilBERT fine-tuning: ~45 min (standard) or ~2 hours (2-stage with MLM)\n",
    "- Ensemble & evaluation: ~1 min\n",
    "\n",
    "## Key Variables & Their Purpose\n",
    "- `df_news`: Main dataset after merging and cleaning\n",
    "- `X_train`, `X_test`: Text data for train/test splits\n",
    "- `y_train`, `y_test`: Binary labels (0=Fake, 1=True)\n",
    "- `vectorizer`: TF-IDF transformer fitted on training data\n",
    "- `X_train_tfidf`, `X_test_tfidf`: Sparse TF-IDF feature matrices\n",
    "- `modellr`, `modeldt`, `rf`: Baseline models (LogReg, DecisionTree, RandomForest)\n",
    "- `model`, `tokenizer`, `trainer`: DistilBERT components\n",
    "- `ensemble_proba`, `ensemble_pred`: Combined predictions from transformer + RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c1cb7",
   "metadata": {},
   "source": [
    "# Code Examples\n",
    "\n",
    "## Loading Saved Models\n",
    "```python\n",
    "from joblib import load\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load baseline model\n",
    "baseline = load('best_baseline_model.joblib')\n",
    "vectorizer = load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load transformer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./distilbert_fakenews')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./distilbert_fakenews')\n",
    "```\n",
    "\n",
    "## Making Predictions on New Text\n",
    "```python\n",
    "new_text = \"Breaking: Scientists discover new planet in solar system\"\n",
    "\n",
    "# Clean text (use clean_text function from notebook)\n",
    "cleaned = clean_text(new_text)\n",
    "\n",
    "# Baseline prediction\n",
    "text_tfidf = vectorizer.transform([cleaned])\n",
    "baseline_pred = baseline.predict(text_tfidf)[0]\n",
    "\n",
    "# Transformer prediction\n",
    "inputs = tokenizer(new_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "outputs = model(**inputs)\n",
    "transformer_pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "print(f\"Baseline: {'Fake' if baseline_pred == 0 else 'True'}\")\n",
    "print(f\"Transformer: {'Fake' if transformer_pred == 0 else 'True'}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cfc62",
   "metadata": {},
   "source": [
    "# Common Issues & Solutions\n",
    "\n",
    "## Issue: \"ModuleNotFoundError: No module named 'google-api-python-client'\"\n",
    "**Solution**: Install correct package name:\n",
    "```bash\n",
    "pip install google-api-python-client\n",
    "```\n",
    "\n",
    "## Issue: \"RuntimeError: MPS backend out of memory\"\n",
    "**Solution**: Reduce batch size in TrainingArguments:\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,  # Reduce from 16\n",
    "    per_device_eval_batch_size=8,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "## Issue: \"Perfect test accuracy (100%)\"\n",
    "**Solution**: Likely data leakage! Check:\n",
    "1. Duplicates in train/test: `train_hashes & test_hashes`\n",
    "2. Publisher names in features: Add to custom_stopwords\n",
    "3. Temporal leakage: Ensure chronological split if data has timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73fef7",
   "metadata": {},
   "source": [
    "# Performance Metrics Interpretation\n",
    "\n",
    "## Baseline Models (Expected Ranges)\n",
    "- **Logistic Regression**: Accuracy ~92-95%, F1 ~0.92-0.95, AUC ~0.95-0.98\n",
    "- **Decision Tree**: Accuracy ~88-92%, F1 ~0.88-0.92, AUC ~0.90-0.94\n",
    "- **Random Forest**: Accuracy ~93-96%, F1 ~0.93-0.96, AUC ~0.96-0.99\n",
    "\n",
    "## Transformer Models (Expected Ranges)\n",
    "- **DistilBERT (standard)**: Accuracy ~97-99%, F1 ~0.97-0.99, AUC ~0.99+\n",
    "- **DistilBERT (2-stage)**: Accuracy ~98-99.5%, F1 ~0.98-0.995, AUC ~0.995+\n",
    "\n",
    "## Ensemble (Expected Improvement)\n",
    "- Typically +0.5-1% accuracy over best individual model\n",
    "- More robust to adversarial examples and edge cases\n",
    "- Better calibrated probabilities for uncertainty estimation\n",
    "\n",
    "## Red Flags\n",
    "- **Accuracy > 99.5%**: Possible data leakage\n",
    "- **Train accuracy >> Test accuracy**: Overfitting (reduce max_features, increase regularization)\n",
    "- **F1 << Accuracy**: Class imbalance (use balanced class weights)\n",
    "- **AUC < Accuracy**: Probability calibration issues (use CalibratedClassifierCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa841d3c",
   "metadata": {},
   "source": [
    "# Project Structure\n",
    "\n",
    "```\n",
    "FakeScope/\n",
    "├── Development.ipynb              # Main training notebook\n",
    "├── LLM_Pipeline.ipynb            # Production inference pipeline\n",
    "├── guide.ipynb                   # This guide\n",
    "├── requirements.txt              # Python dependencies\n",
    "├── .gitignore                    # Git exclusions\n",
    "├── datasets/\n",
    "│   └── input/\n",
    "│       ├── alt/News.csv\n",
    "│       └── alt 2/New Task.csv\n",
    "├── best_baseline_model.joblib    # Saved baseline (created after training)\n",
    "├── tfidf_vectorizer.joblib       # Saved vectorizer\n",
    "├── distilbert_fakenews/          # Saved transformer model\n",
    "│   ├── config.json\n",
    "│   ├── model.safetensors\n",
    "│   ├── tokenizer_config.json\n",
    "│   ├── tokenizer.json\n",
    "│   └── vocab.txt\n",
    "├── distilbert_news_adapted/      # Domain-adapted model (2-stage)\n",
    "├── results/                      # Training checkpoints\n",
    "├── mlm_results/                  # MLM training outputs\n",
    "└── factcheck_cache.json          # Fact-check API cache\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
