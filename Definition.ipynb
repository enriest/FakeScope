{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3212c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each CSV file\n",
    "df_fake = pd.read_csv(\"datasets/input/Fake/Fake.csv\")\n",
    "df_true = pd.read_csv(\"datasets/input/True/True.csv\")\n",
    "df_news = pd.read_csv(\"datasets/input/alt/News.csv\")\n",
    "\n",
    "# Add a label column to each DataFrame\n",
    "df_fake['label'] = 'Fake'\n",
    "df_true['label'] = 'True'\n",
    "df_news['label'] = 'News'\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_combined = pd.concat([df_fake, df_true, df_news], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d330d7",
   "metadata": {},
   "source": [
    "Baseline, transformer, and ensemble refer to different concepts in machine learning models with distinct purposes and complexities.\n",
    "\n",
    "Baseline models are simple, basic models used as a reference point to evaluate if more complex models improve performance. They often predict the majority class, random guesses, or simple statistics like the mean. Their goal is to set a minimum performance threshold that more advanced models must exceed to justify their complexity and computational cost [1][2][3].\n",
    "\n",
    "Transformer models are sophisticated deep learning architectures primarily used in natural language processing and other sequential data tasks. They rely on a self-attention mechanism that allows them to process entire sequences in parallel and capture long-range dependencies more efficiently compared to older models like RNNs or LSTMs. This makes transformers highly effective for tasks requiring context understanding over long sequences [4][5][6].\n",
    "\n",
    "Ensemble models combine multiple individual machine learning models, called base learners, to improve prediction accuracy and robustness. By aggregating the outputs of diverse models, ensembles reduce error, variance, and bias, often outperforming single models. Common ensemble techniques include bagging, boosting, and stacking, aiming to leverage complementary strengths of different models [7][8][9][10].\n",
    "\n",
    "In summary:\n",
    "\n",
    "| Model Type   | Purpose                            | Characteristics                                     | Use Cases                             |\n",
    "|--------------|----------------------------------|---------------------------------------------------|-------------------------------------|\n",
    "| Baseline     | Performance benchmark             | Simple, often naive or statistical prediction     | Initial step in model development    |\n",
    "| Transformer  | Handling complex sequential data | Uses self-attention for context and parallel processing | NLP, speech, vision, long sequences  |\n",
    "| Ensemble     | Improve accuracy and robustness  | Combines multiple models to reduce errors         | Diverse ML tasks needing high accuracy|\n",
    "\n",
    "These three serve different roles from baseline comparison, advanced sequence modeling, to performance boosting through combination. Each is chosen depending on the problem complexity and performance requirements [1][4][7][9].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6830172",
   "metadata": {},
   "source": [
    "From PAPER11\n",
    "\n",
    "Short answer: The paragraph describes the standard BERT training loop and you mostly follow it in this notebook.\n",
    "\n",
    "Mapping (paragraph → your notebook)\n",
    "\n",
    "\"multiple epochs\" — yes (TrainingArguments.num_train_epochs set, e.g. 3).\n",
    "\"AdamW optimizer\" — Trainer uses AdamW by default, so yes (you didn't override it).\n",
    "\"linear learning rate scheduler\" — Trainer defaults to a linear scheduler; you haven't overridden lr_scheduler_type, so it's effectively used (warmup_steps defaults to 0 unless set).\n",
    "\"backpropagation to adjust weights\" — handled automatically by Trainer.train() (yes).\n",
    "\"periodically evaluated on a validation dataset, assessing accuracy and generating classification reports\" — you set eval_strategy='epoch' and compute_metrics returns accuracy & f1; you also print classification_report for baseline models (but you do not print sklearn's classification_report for the transformer unless you run a predict + report).\n",
    "\"iteratively improves the model\" — Trainer with load_best_model_at_end=True and epoch evaluation implements that workflow.\n",
    "Minor gaps / recommendations\n",
    "\n",
    "You rely on Trainer defaults for optimizer/scheduler. If you want to be explicit, set lr_scheduler_type and warmup_steps (or warmup_ratio) in TrainingArguments.\n",
    "If you want a full classification_report for the fine-tuned transformer, call trainer.predict(...) and print sklearn.metrics.classification_report (not done by compute_metrics).\n",
    "Consider setting logging_steps and disable_tqdm=False during debugging so you see progress live (you already use logging_steps in some places).\n",
    "Optional small edits you can add to the notebook\n",
    "\n",
    "Explicit TrainingArguments (set scheduler + warmup):\n",
    "\n",
    "\n",
    "# ...existing code...training_args = TrainingArguments(    output_dir='./results',    eval_strategy='epoch',    save_strategy='epoch',    learning_rate=2e-5,    per_device_train_batch_size=16,    per_device_eval_batch_size=16,    num_train_epochs=3,    weight_decay=0.01,    load_best_model_at_end=True,    metric_for_best_model='accuracy',    fp16=False,    use_mps_device=True,    lr_scheduler_type='linear',   # explicit    warmup_steps=500,            # example warmup (adjust)    logging_steps=50,    disable_tqdm=False,)# ...existing code...\n",
    "Print full classification report for the transformer:\n",
    "\n",
    "\n",
    "# ...existing code...from sklearn.metrics import classification_reportpreds = trainer.predict(test_dataset)y_true = preds.label_idsy_pred = np.argmax(preds.predictions, axis=1)print(classification_report(y_true, y_pred, digits=4))# ...existing code...\n",
    "If you want, I can add these changes into the notebook (or show exactly which cell(s) to edit).\n",
    "\n",
    "GPT-5 mini • 0x\n",
    "Development.ipynb • Cell 52\n",
    "\n",
    "Ask\n",
    "GPT-5 mini\n",
    " main*\n",
    " 0 \n",
    " 0\n",
    " Connect\n",
    " 0 archivos y 58 celdas para analizar\n",
    "Cell 52 of 69\n",
    "Spaces: 4\n",
    "GPT-5 mini\n",
    "Short answer: The paragraph describes the standard BERT training loop and you mostly follow it in this notebook. Mapping (paragraph → your notebook) - \"multiple epochs\" — yes (TrainingArguments.num_train_epochs set, e.g. 3). - \"AdamW optimizer\" — Trainer uses AdamW by default, so yes (you didn't override it). - \"linear learning rate scheduler\" — Trainer defaults to a linear scheduler; you haven't overridden lr_scheduler_type, so it's effectively used (warmup_steps defaults to 0 unless set). - \"backpropagation to adjust weights\" — handled automatically by Trainer.train() (yes). - \"periodically evaluated on a validation dataset, assessing accuracy and generating classification reports\" — you set eval_strategy='epoch' and compute_metrics returns accuracy & f1; you also print classification_report for baseline models (but you do not print sklearn's classification_report for the transformer unless you run a predict + report). - \"iteratively improves the model\" — Trainer with load_best_model_at_end=True and epoch evaluation implements that workflow. Minor gaps / recommendations - You rely on Trainer defaults for optimizer/scheduler. If you want to be explicit, set lr_scheduler_type and warmup_steps (or warmup_ratio) in TrainingArguments. - If you want a full classification_report for the fine-tuned transformer, call trainer.predict(...) and print sklearn.metrics.classification_report (not done by compute_metrics). - Consider setting logging_steps and disable_tqdm=False during debugging so you see progress live (you already use logging_steps in some places). Optional small edits you can add to the notebook Explicit TrainingArguments (set scheduler + warmup): # ...existing code... training_args = TrainingArguments( output_dir='./results', eval_strategy='epoch', save_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=3, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model='accuracy', fp16=False, use_mps_device=True, lr_scheduler_type='linear', # explicit warmup_steps=500, # example warmup (adjust) logging_steps=50, disable_tqdm=False, ) # ...existing code...Print full classification report for the transformer: # ...existing code... from sklearn.metrics import classification_report preds = trainer.predict(test_dataset) y_true = preds.label_ids y_pred = np.argmax(preds.predictions, axis=1) print(classification_report(y_true, y_pred, digits=4)) # ...existing code...If you want, I can add these changes into the notebook (or show exactly which cell(s) to edit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f496a92",
   "metadata": {},
   "source": [
    "Basic Steps to Implement Google Fact Check API:\n",
    "\t1.\tUse the Google Fact Check Tools API to query claims related to your text input.\n",
    "\t2.\tSend a search query with your claim text to the API and get fact check results.\n",
    "\t3.\tExtract relevant details from the API response such as claim text, verdict, publisher, and review date.\n",
    "\t4.\tCompare your Robert model’s prediction with the fact-check verdict returned from the API.\n",
    "\t5.\tOptionally, aggregate the comparisons for analysis or final decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec4e92",
   "metadata": {},
   "source": [
    "Docker is an open platform for developing, shipping, and running applications inside lightweight, portable containers. These containers bundle your application code together with all its dependencies and runtime environment, ensuring that the app runs consistently across different machines and environments without issues caused by differences in software configurations or operating systems [1][2].\n",
    "\n",
    "### How Docker Works\n",
    "- Docker containers run isolated from one another but share the host OS kernel.\n",
    "- Containers are lightweight compared to full virtual machines because they don't require a complete OS instance.\n",
    "- Docker images define the environment and software stack for a container, which you create using a Dockerfile.\n",
    "- Containers can be run on any system with Docker installed, enabling easy distribution and deployment.\n",
    "\n",
    "### Integrating Docker into Your Project Flow\n",
    "1. **Containerize your application**: Write a Dockerfile that specifies your app code, dependencies, and how to run it. For instance, for a Python project, your Dockerfile would install Python, required libraries, copy your code, and specify the startup command.\n",
    "2. **Build a Docker image**: Use the Dockerfile to build an image that packages your environment and app.\n",
    "3. **Run your app inside a container**: The image is instantiated as a container on any machine that has Docker installed, running your app reliably.\n",
    "4. **Integrate with ML workflow**: Containerize your Robert model service and the Google Fact Check API integration code to ensure consistent deployment across development, testing, and production.\n",
    "5. **Orchestrate and scale**: Use tools like Docker Compose to manage multi-container setups (e.g., model + API + database). For large scale, container orchestrators like Kubernetes can be employed.\n",
    "\n",
    "### Benefits for Your FakeScope Project\n",
    "- Ensures your Robert model and API integration run identically on any platform.\n",
    "- Simplifies dependency management and environment setup for your team.\n",
    "- Eases deployment on cloud servers or other production environments.\n",
    "- Enables automated testing and CI/CD pipelines inside containers.\n",
    "\n",
    "### Basic Example Dockerfile for Your Python Project\n",
    "```Dockerfile\n",
    "# Use official lightweight Python image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory inside container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy dependency list and install\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy project source code\n",
    "COPY . .\n",
    "\n",
    "# Command to run your Python app (replace with your entry point)\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "With this, you can build the image using:\n",
    "```bash\n",
    "docker build -t fakescope-app .\n",
    "```\n",
    "and run your container with:\n",
    "```bash\n",
    "docker run -p 5000:5000 fakescope-app\n",
    "```\n",
    "\n",
    "Using Docker in this way streamlines your development, testing, and deployment processes, making your FakeScope project more portable, scalable, and robust [1][3][4][5].\n",
    "\n",
    "Fuentes\n",
    "[1] What is Docker? https://docs.docker.com/get-started/docker-overview/\n",
    "[2] Docker (software) https://en.wikipedia.org/wiki/Docker_(software)\n",
    "[3] Step-by-Step Guide to Deploying ML Models with Docker https://www.kdnuggets.com/step-by-step-guide-to-deploying-ml-models-with-docker\n",
    "[4] Getting Started with Docker for AI/ML: A Beginner's Guide https://dev.to/docker/getting-started-with-docker-for-aiml-a-beginners-guide-4k6j\n",
    "[5] 10 Docker Project Ideas: From Beginner to Advanced https://www.datacamp.com/blog/docker-projects\n",
    "[6] Docker: Accelerated Container Application Development https://www.docker.com\n",
    "[7] Please, someone explain Docker to me like I am an idiot. https://www.reddit.com/r/docker/comments/keq9el/please_someone_explain_docker_to_me_like_i_am_an/\n",
    "[8] What is Docker? https://www.geeksforgeeks.org/devops/introduction-to-docker/\n",
    "[9] What Is Docker? https://www.ibm.com/think/topics/docker\n",
    "[10] Running Python on Docker - Earthly Blog https://earthly.dev/blog/python-docker/\n",
    "[11] What is Docker? | AWS https://aws.amazon.com/docker/\n",
    "[12] MLOps Workflow for Docker-Based AI Model Deployment https://www.runpod.io/articles/guides/mlops-workflow-docker-ai-deployment\n",
    "[13] How to “Dockerize” Your Python Applications https://www.docker.com/blog/how-to-dockerize-your-python-applications/\n",
    "[14] What is a Container? https://www.docker.com/resources/what-container/\n",
    "[15] Your First Containerized Machine Learning Deployment ... https://machinelearningmastery.com/your-first-containerized-machine-learning-deployment-with-docker-and-fastapi/\n",
    "[16] The best way to use docker with Python for development ... https://www.reddit.com/r/docker/comments/1b768ft/the_best_way_to_use_docker_with_python_for/\n",
    "[17] Qu'est-ce que Docker ? Fonctionnement, avantages, limites https://about.gitlab.com/fr-fr/blog/what-is-docker-comprehensive-guide/\n",
    "[18] Machine Learning Inside the Container https://www.docker.com/resources/machine-learning-inside-the-container-dockercon-2023/\n",
    "[19] Docker Best Practices for Python Developers https://testdriven.io/blog/docker-best-practices/\n",
    "[20] What is Docker? https://opensource.com/resources/what-docker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ff93d",
   "metadata": {},
   "source": [
    "Docker containers and virtual machines (VMs) are both used to create isolated environments for running applications, but they differ significantly in architecture, resource usage, performance, and use cases. Here is a comparison tailored to your FakeScope project with the Robert model and Google Fact Check API integration:\n",
    "\n",
    "| Aspect                | Docker Containers                                    | Virtual Machines (VMs)                          |\n",
    "|-----------------------|------------------------------------------------------|------------------------------------------------|\n",
    "| **Architecture**       | Share the host OS kernel; run as isolated processes  | Include a full guest OS on top of the host OS  |\n",
    "| **Resource Usage**     | Lightweight, low overhead                             | Heavyweight, higher overhead due to guest OS   |\n",
    "| **Startup Time**       | Fast (seconds or less)                               | Slow (minutes sometimes)                        |\n",
    "| **Portability**        | Highly portable; runs anywhere Docker is installed  | Portable but requires compatible hypervisor    |\n",
    "| **Performance**        | Near-native performance                              | Slightly slower due to full OS virtualization  |\n",
    "| **Isolation Level**    | Process-level isolation                               | Stronger isolation with complete OS             |\n",
    "| **Use Case Fit**       | Ideal for microservices, apps, ML model deployment  | Useful when full OS customization needed       |\n",
    "| **Management**         | Easier to manage and orchestrate (e.g., Kubernetes) | More complex in orchestration                    |\n",
    "| **Disk Usage**         | Smaller images and snapshots                         | Larger images due to full OS                      |\n",
    "| **Security**           | More vulnerable to kernel exploits (shared kernel)  | Better isolation from host OS                     |\n",
    "\n",
    "### For Your FakeScope Project:\n",
    "- **Docker** is a better fit because your Robert model and API integration likely don’t need full OS virtualization. Docker containers will run lightweight, fast, and consistent environments across development and production.\n",
    "- Docker simplifies dependencies and environment management and fits well with CI/CD workflows for machine learning model deployment and API services.\n",
    "- Using Docker promotes scalability and microservices architecture, making it easier to update components like the Robert model or API independently.\n",
    "- VMs could be considered if your project has strict security requirements or needs to run on different OS types simultaneously, but this adds complexity and resource demands.\n",
    "\n",
    "### Summary\n",
    "Docker containers offer an efficient, portable, and scalable solution for your FakeScope deployment. VMs provide stronger isolation but with higher overhead and slower startup times, which may be unnecessary for your Python-based model and API integration [1][2][3][4].\n",
    "\n",
    "Fuentes\n",
    "[1] What is Docker? https://docs.docker.com/get-started/docker-overview/\n",
    "[2] Docker (software) https://en.wikipedia.org/wiki/Docker_(software)\n",
    "[3] Step-by-Step Guide to Deploying ML Models with Docker https://www.kdnuggets.com/step-by-step-guide-to-deploying-ml-models-with-docker\n",
    "[4] MLOps Workflow for Docker-Based AI Model Deployment https://www.runpod.io/articles/guides/mlops-workflow-docker-ai-deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547eabb4",
   "metadata": {},
   "source": [
    "API KEY GOOGLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PYTHON DE OPEN AI PLATFORM\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Analyze the letter and provide a summary of the key points.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_url\": \"https://www.berkshirehathaway.com/letters/2024ltr.pdf\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc90a9e",
   "metadata": {},
   "source": [
    "The text you're referring to describes the **training process for transformer models** (specifically DistilBERT) in your Project.ipynb notebook.\n",
    "\n",
    "## Where This Appears in Your Notebook\n",
    "\n",
    "This relates to **Part V: Transformer Models (Deep Learning)** in your Project.ipynb, specifically:\n",
    "\n",
    "### Cell 72 (Stage 2 Fine-tuning)\n",
    "Where you fine-tune DistilBERT on the fake news classification task. The code uses:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_args = TrainingArguments(\n",
    "    output_dir='./results_2stage',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-5,        # ← Learning rate for AdamW optimizer\n",
    "    weight_decay=0.01,\n",
    "    ...\n",
    ")\n",
    "\n",
    "stage2_trainer = Trainer(\n",
    "    model=domain_adapted_model,\n",
    "    args=stage2_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,  # ← Calculates accuracy, F1, etc.\n",
    ")\n",
    "\n",
    "stage2_trainer.train()  # ← Runs the training loop with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c42eb3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What's Happening Behind the Scenes\n",
    "\n",
    "1. **AdamW Optimizer**: HuggingFace Trainer uses AdamW by default (Adam with weight decay)\n",
    "2. **Linear LR Scheduler**: Learning rate decreases linearly from `2e-5` to 0 over training\n",
    "3. **Backpropagation**: Automatic via PyTorch - computes gradients and updates weights\n",
    "4. **Loss Calculation**: Cross-entropy loss comparing predictions vs. actual labels (0=Fake, 1=True)\n",
    "5. **Validation**: `eval_strategy='epoch'` means it evaluates after each epoch using `compute_metrics`\n",
    "\n",
    "### The `compute_metrics` Function\n",
    "\n",
    "Earlier in your notebook, you should have defined this function (or it's implied):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01104ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e479e20",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is what generates the classification reports and accuracy metrics during training.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary**: That description is explaining what the HuggingFace `Trainer` class does automatically when you call `stage2_trainer.train()` in your transformer training cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f7daf",
   "metadata": {},
   "source": [
    "Created 5 todos\n",
    "\n",
    "Searched text for `ROC Curve Comparison` (`**/Project.ipynb`), 2 results\n",
    "\n",
    "Summarized conversation history## Conversation recap (focused on recent actions)\n",
    "\n",
    "### High-level arc\n",
    "You moved from classical baseline optimization (TF‑IDF + traditional classifiers) into:\n",
    "1. Adding boosted tree models (XGBoost, LightGBM) with efficient randomized hyperparameter search.\n",
    "2. Fixing critical transformer training issues (missing `compute_metrics`, tokenizer/name mismatches, model save logic).\n",
    "3. Expanding evaluation: unified performance table, ROC/AUC visualization across all models (baselines + transformer + ensemble).\n",
    "4. Planning to enrich metrics further with Precision‑Recall (PR) curves (currently pending).\n",
    "\n",
    "### Most recent completed actions\n",
    "- Integrated XGBoost (with `tree_method='hist'`) and LightGBM using `RandomizedSearchCV (n_iter=20, scoring='f1')` to reduce tuning time.\n",
    "- Adjusted TF‑IDF vectorizer (e.g., `max_features=3000`) for a leaner sparse feature space.\n",
    "- Added a markdown explanation justifying XGBoost & LightGBM (speed, regularization, sparsity handling, interpretability).\n",
    "- Created a comparative baseline performance summary identifying Logistic Regression as the strongest classical model before transformer integration.\n",
    "- Implemented DistilBERT two‑stage rationale: Stage 1 domain MLM adaptation → Stage 2 supervised classification.\n",
    "- Fixed `NameError: compute_metrics` by defining a metrics function (accuracy, precision, recall, F1) in the Stage 2 Trainer cell.\n",
    "- Corrected model saving cell variable references (`stage2_trainer` and `mlm_tokenizer`) preventing runtime errors.\n",
    "- Repaired cross‑validation cell (tokenizer name alignment and memory cleanup with `torch.mps.empty_cache()`).\n",
    "- Built a consolidated evaluation cell:\n",
    "  - Extracted probabilities from all available models (logreg, decision tree, random forest, XGBoost, LightGBM, DistilBERT, ensemble).\n",
    "  - Added robust fallbacks if a model object is missing (neutral probabilities).\n",
    "  - Generated ROC curves and computed AUC for every model; plotted multi‑model ROC comparison.\n",
    "  - Assembled a performance DataFrame (accuracy, precision, recall, F1, AUC) and visual bar chart.\n",
    "- Established a task plan to add Precision‑Recall curves (not yet implemented).\n",
    "\n",
    "### Key fixes & why they mattered\n",
    "- `compute_metrics` addition: Enables Trainer evaluation loop—without it, supervised metrics wouldn’t populate.\n",
    "- Variable naming consistency (`mlm_tokenizer`, `stage2_trainer`): Prevents silent failures in loading/saving and avoids accidental retraining or mis-saved artifacts.\n",
    "- Probability extraction robustness: Ensures downstream ensemble and ROC logic doesn’t crash if a single model wasn’t executed earlier in the notebook.\n",
    "- RandomizedSearch over GridSearch for boosted models: Cuts search time significantly while still exploring broad hyperparameter space.\n",
    "\n",
    "### Current project state\n",
    "- Classical + boosted + transformer + ensemble comparison operational.\n",
    "- ROC curves implemented and validated.\n",
    "- Two-stage DistilBERT pipeline functioning with metrics.\n",
    "- Explanation and justification markdown cells present.\n",
    "- PR curve visualization pending (precision_recall_curve & average_precision_score not yet added).\n",
    "\n",
    "### Pending (next actionable steps)\n",
    "1. Implement Precision‑Recall curves:\n",
    "   - Import: `from sklearn.metrics import precision_recall_curve, average_precision_score`\n",
    "   - Compute per model: precision, recall, thresholds.\n",
    "   - Plot all PR curves with an added baseline (no-skill horizontal line at positive class prevalence).\n",
    "   - Add Average Precision (AP) to legend or a summary table.\n",
    "2. Confirm class distribution and include prevalence annotation to contextualize PR curves.\n",
    "3. (Optional) Add confidence intervals for AP similar to ROC if cross‑validation scores are available.\n",
    "4. (Optional) Persist the comparison metrics (CSV/JSON) for reproducibility and downstream reporting.\n",
    "\n",
    "### Value of upcoming PR curves\n",
    "- Better discrimination insight under potential class imbalance (fake vs true).\n",
    "- Highlights precision trade-offs at high recall zones—important for minimizing false positives if fact-checking pipeline escalates claims.\n",
    "\n",
    "### Suggested small adjacent improvements after PR curves\n",
    "- Add calibration check (e.g., reliability diagram / Brier score) for ensemble and transformer probabilities.\n",
    "- Log metrics to MLflow for unified tracking (if not already for boosted models).\n",
    "- Include confusion matrix heatmaps for final chosen model set (transformer + best classical + ensemble).\n",
    "- Lightweight unit test to assert probability arrays shape consistency before plotting.\n",
    "\n",
    "### Potential edge cases to keep in mind\n",
    "- Missing trained artifacts (cells skipped) → keep fallbacks.\n",
    "- Class imbalance shifts if new data ingested → recompute baseline prevalence used in PR baseline.\n",
    "- Token length >512 truncation effects on DistilBERT—consider reporting mean truncation ratio.\n",
    "- Memory pressure on MPS during multi‑model evaluation—clear caches post-transformer inference if needed.\n",
    "\n",
    "### Quick checklist snapshot\n",
    "- TF‑IDF optimization: done\n",
    "- Boosted models added & tuned: done\n",
    "- Explanatory markdown cells: done\n",
    "- DistilBERT pipeline fixes: done\n",
    "- Unified performance + ROC: done\n",
    "- Precision‑Recall curves: pending\n",
    "\n",
    "### Next immediate move (if you request it)\n",
    "Add the PR curve logic into the final comparison cell and regenerate plots + AP summary.\n",
    "\n",
    "Let me know when you’d like me to proceed with implementing the Precision‑Recall curves, or if you want any part of this recap distilled further."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
