{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5002ca",
   "metadata": {},
   "source": [
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\"language\": \"markdown\"},\n",
    "      \"source\": [\n",
    "        \"# FakeScope Development.ipynb - Complete Guide\",\n",
    "        \"\",\n",
    "        \"## Overview\",\n",
    "        \"This notebook implements a comprehensive fake news detection pipeline combining traditional ML (TF-IDF + baselines) with transformer models (DistilBERT) and ensemble methods.\",\n",
    "        \"\",\n",
    "        \"## Chronology & Architecture\",\n",
    "        \"\",\n",
    "        \"### Phase 1: Data Loading & Preprocessing (Cells 1-17)\",\n",
    "        \"- **Load datasets**: Two CSV files (Fake.csv, True.csv) merged into unified dataset\",\n",
    "        \"- **Label normalization**: Maps various labels to binary (0=Fake, 1=True)\",\n",
    "        \"- **Text cleaning**: Removes punctuation, digits, stopwords (NLTK + sklearn + custom)\",\n",
    "        \"- **Deduplication**: MD5 content hashes to detect exact duplicates\",\n",
    "        \"- **EDA**: WordClouds, class distribution, top word/bigram frequencies\",\n",
    "        \"\",\n",
    "        \"**Key Configuration**:\",\n",
    "        \"```python\",\n",
    "        \"TOKEN_PATTERN = r'(?u)\\\\b\\\\w\\\\w+\\\\b'  # Skip 1-letter tokens\",\n",
    "        \"MIN_DF = 5                           # Drop very rare tokens\",\n",
    "        \"MAX_DF = 0.90                        # Drop extremely common tokens\",\n",
    "        \"NGRAM_RANGE = (1, 2)                 # Unigrams and bigrams\",\n",
    "        \"```\",\n",
    "        \"\",\n",
    "        \"### Phase 2: Train/Test Split (Cells 18-19)\",\n",
    "        \"- **GroupShuffleSplit**: Prevents data leakage by grouping duplicate articles\",\n",
    "        \"- **25% test split** with random_state=42 for reproducibility\",\n",
    "        \"- **Content hash grouping**: Ensures same article variants stay in same split\",\n",
    "        \"\",\n",
    "        \"### Phase 3: Feature Extraction (Cell 20)\",\n",
    "        \"- **TF-IDF vectorization**: max_features=5000, custom stopwords\",\n",
    "        \"- **Train-only fitting**: Vectorizer fit on X_train, transform on X_test\",\n",
    "        \"- **Output**: Sparse matrices (train: ~75% samples, test: ~25%)\",\n",
    "        \"\",\n",
    "        \"### Phase 4: Baseline Models (Cells 21-34)\",\n",
    "        \"- **Logistic Regression**: Linear baseline with L2 regularization\",\n",
    "        \"- **Decision Tree**: Non-linear baseline with max_depth tuning\",\n",
    "        \"- **Random Forest**: Ensemble baseline with GridSearchCV hyperparameter tuning\",\n",
    "        \"- **Evaluation**: Accuracy, F1, ROC/AUC, confusion matrices, feature importance\",\n",
    "        \"- **Best model selection**: Choose by AUC, save to best_baseline_model.joblib\",\n",
    "        \"\",\n",
    "        \"### Phase 5: Transformer Models (Cells 35-56)\",\n",
    "        \"- **DistilBERT fine-tuning**: HuggingFace Transformers with custom tokenization\",\n",
    "        \"- **2-stage training** (optional):\",\n",
    "        \"  1. **Stage 1**: Masked Language Modeling (MLM) on unlabeled corpus for domain adaptation\",\n",
    "        \"  2. **Stage 2**: Fine-tune adapted model on labeled fake news classification\",\n",
    "        \"- **Training config**: 3 epochs, batch_size=16, learning_rate=2e-5, MPS (Apple Silicon)\",\n",
    "        \"- **Cross-validation**: 5-fold StratifiedKFold for robust evaluation\",\n",
    "        \"- **Model persistence**: Save to ./distilbert_fakenews/\",\n",
    "        \"\",\n",
    "        \"### Phase 6: Ensemble & Error Analysis (Cells 39-42)\",\n",
    "        \"- **Soft voting ensemble**:\",\n",
    "        \"  ```python\",\n",
    "        \"  ensemble_proba = 0.6 * bert_proba + 0.4 * rf_proba\",\n",
    "        \"  ensemble_pred = (ensemble_proba > 0.5).astype(int)\",\n",
    "        \"  ```\",\n",
    "        \"- **Weight rationale**: 60% transformer (semantic context) + 40% RF (robust features)\",\n",
    "        \"- **Error analysis**: Identify misclassified examples, show prediction details\",\n",
    "        \"- **Attention visualization**: Use bertviz to understand model focus\",\n",
    "        \"\",\n",
    "        \"### Phase 7: Fact-Checking Integration (Cells 58-70)\",\n",
    "        \"- **Google Fact Check API**: Query external fact-checkers for claims\",\n",
    "        \"- **Claim extraction**: Use spaCy to extract sentences >10 words\",\n",
    "        \"- **Credibility scoring**: Combine model predictions with fact-check verdicts\",\n",
    "        \"- **Comment generation**: T5/FLAN-style explanations based on score\",\n",
    "        \"\",\n",
    "        \"## Key Features & Best Practices\",\n",
    "        \"\",\n",
    "        \"### Data Leakage Prevention\",\n",
    "        \"1. **Content hash grouping**: Prevents train/test contamination from duplicates\",\n",
    "        \"2. **Train-only vectorizer fitting**: TF-IDF vocabulary built only from training data\",\n",
    "        \"3. **No publisher names in features**: Custom stopwords remove 'reuters', 'ap', etc.\",\n",
    "        \"\",\n",
    "        \"### Model Reproducibility\",\n",
    "        \"- Fixed random_state=42 across all splits and models\",\n",
    "        \"- Saved models: best_baseline_model.joblib, tfidf_vectorizer.joblib, distilbert_fakenews/\",\n",
    "        \"- Training logs: trainer_state.json tracks loss/accuracy per epoch\",\n",
    "        \"\",\n",
    "        \"### Performance Optimization\",\n",
    "        \"- MPS device support for Apple Silicon GPU acceleration\",\n",
    "        \"- Batch processing for transformer inference\",\n",
    "        \"- Sparse matrix TF-IDF for memory efficiency\",\n",
    "        \"\",\n",
    "        \"## Configuration Flags\",\n",
    "        \"\",\n",
    "        \"### REBUILD_SPLIT_DEDUP (not yet implemented)\",\n",
    "        \"Set to True to automatically deduplicate and regenerate train/test splits.\",\n",
    "        \"\",\n",
    "        \"### REFIT_VECTORIZE_TRAIN_ONLY (not yet implemented)\",\n",
    "        \"Set to True to enforce strict train-only vectorizer fitting with validation checks.\"\n",
    "      ]\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbfd2c",
   "metadata": {},
   "source": [
    "# Practical Usage Guide for Development.ipynb\n",
    "\n",
    "## Quick Start\n",
    "1. **Install dependencies**: See requirements.txt for all needed packages.\n",
    "2. **Select Python interpreter**: Use a virtual environment and select it in VS Code.\n",
    "3. **Run cells sequentially**: Start from the top, executing each cell in order.\n",
    "4. **Configure flags**: Set REBUILD_SPLIT_DEDUP and REFIT_VECTORIZE_TRAIN_ONLY as needed for your workflow.\n",
    "5. **Monitor outputs**: Check printed warnings for data leakage, model performance, and errors.\n",
    "6. **Save models**: Use provided cells to save/load trained models for reproducibility.\n",
    "7. **Troubleshooting**: If you encounter errors, use the safe_run wrapper and review error messages. Restart the kernel if needed.\n",
    "\n",
    "## Example: Data Deduplication\n",
    "```python\n",
    "REBUILD_SPLIT_DEDUP = True  # Enable deduplication and split rebuild\n",
    "# Run the deduplication cell to remove duplicate articles before splitting\n",
    "```\n",
    "## Example: Vectorizer Integrity\n",
    "```python\n",
    "REFIT_VECTORIZE_TRAIN_ONLY = True  # Fit TF-IDF only on training data\n",
    "# Run the vectorizer integrity cell to check for test-only tokens\n",
    "```\n",
    "## Example: Ensemble Prediction\n",
    "```python\n",
    "ensemble_proba = 0.6 * bert_proba + 0.4 * rf_proba\n",
    "ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "# Use ensemble_pred for final predictions\n",
    "```\n",
    "## Example: Error Recovery\n",
    "```python\n",
    "result = safe_run(trainer.train, error_msg=\"Training failed\", fallback=None)\n",
    "```\n",
    "## Example: Model Saving/Loading\n",
    "```python\n",
    "# Save model\n",
    "trainer.save_model('./distilbert_fakenews')\n",
    "tokenizer.save_pretrained('./distilbert_fakenews')\n",
    "# Load model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('./distilbert_fakenews')\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('./distilbert_fakenews')\n",
    "```\n",
    "## Troubleshooting Tips\n",
    "- If you see 'ModuleNotFoundError', check your Python environment and install missing packages with pip.\n",
    "- If you get memory errors, reduce batch size or max_features in TF-IDF.\n",
    "- For API errors, check your keys and network connection.\n",
    "- Restart the kernel if you encounter persistent issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055dde49",
   "metadata": {},
   "source": [
    "# Updating requirements.txt\n",
    "\n",
    "Recommended requirements (pin versions for reproducibility):\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea7989",
   "metadata": {},
   "source": [
    "# Updating .gitignore\n",
    "\n",
    "Recommended entries for Python/ML projects:\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
